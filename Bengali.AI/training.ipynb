{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n",
    "import sklearn.metrics\n",
    "import scipy.special\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import h5py\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_gen = np.random.RandomState(41)\n",
    "\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "\n",
    "VALIDATION_SPLIT = 98\n",
    "TEST_SPLIT = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with dataset we need to have train/val/test datasets in hdf5 files\n",
    "use cases:\n",
    "\n",
    "1. local/colab testing and architecture tuning - train on train, then validate on val and measure final score on test\n",
    "2. while training final model for submittion - mixup all 3 \n",
    "3. while ensembling - use val as train for ensemble and test to verify score\n",
    "\n",
    "\n",
    "4. would be nice to keep class distibution across the datasets, but this can be done later\n",
    "\n",
    "hdf format is 2 datasets, 1st with pictures, 2nd with labels (same shape across 1 axis)\n",
    "split size for train could be the same as original files, excluding test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_files = [str.format('Data/bengaliai-cv19/train_image_data_{0}.parquet',i) for i in range(4) ]\n",
    "\n",
    "\n",
    "# train_df = pd.read_csv('Data/bengaliai-cv19/train.csv')\n",
    "# y_labels = train_df.image_id.copy()\n",
    "# y_1 = pd.get_dummies(train_df.vowel_diacritic).values\n",
    "# y_2 = pd.get_dummies(train_df.grapheme_root).values\n",
    "# y_3 = pd.get_dummies(train_df.consonant_diacritic).values\n",
    "# y = np.hstack([y_1,y_2,y_3])\n",
    "# assert(y.shape[1]==11+168+7)\n",
    "# del train_df, y_1,y_2,y_3\n",
    "# gc.collect()\n",
    "# assert (y_labels.shape[0]==y.shape[0])\n",
    "# n_samples = y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at this point we know the whole DS contains y.shape[0] = y_labels.shape[0] examples and we can split those accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_indexes = rand_gen.permutation(y.shape[0])\n",
    "# train_indexes = shuffled_indexes[:n_samples*VALIDATION_SPLIT//100]\n",
    "# valid_indexes = shuffled_indexes[n_samples*VALIDATION_SPLIT//100:n_samples*TEST_SPLIT//100]\n",
    "# test_indexes  = shuffled_indexes[n_samples*TEST_SPLIT//100:]\n",
    "\n",
    "# valid_images = []\n",
    "# valid_labels = []\n",
    "# test_images = []\n",
    "# test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     chunk_train_indexes = np.compress((train_indexes>=i*50210) & (train_indexes<(i+1)*50210),train_indexes)\n",
    "#     chunk_valid_indexes = np.compress((valid_indexes>=i*50210) & (valid_indexes<(i+1)*50210),valid_indexes)\n",
    "#     chunk_test_indexes = np.compress((test_indexes>=i*50210) & (test_indexes<(i+1)*50210),test_indexes)\n",
    "\n",
    "#     print('loading '+train_files[i])\n",
    "#     images = pd.read_parquet(train_files[i])\n",
    "#     # we'd better use numpy indexing instead of combining numpy and pandas (not sure they are 100% consistent)\n",
    "#     # float 16 is enough to save from parquet to hdf5\n",
    "#     all_chunk_images = images.iloc[:,1:].values.astype(np.float16).reshape(-1, HEIGHT, WIDTH)/255.0\n",
    "#     del images\n",
    "#     gc.collect()\n",
    "\n",
    "#     # saving train, split by 2 files each\n",
    "    \n",
    "#     train_chunk_images = all_chunk_images[chunk_train_indexes-50210*i] # train\n",
    "#     train_y_chuck = y[chunk_train_indexes]\n",
    "#     assert(train_y_chuck.shape[0]==train_chunk_images.shape[0])\n",
    "    \n",
    "#     train_split_idx = train_y_chuck.shape[0]//2\n",
    "\n",
    "#     ## Not dry, but can live with it, as it's one time thing\n",
    "#     train_chunk_0_fname = str.format('Data/bengaliai-cv19/train_image_data_processed{0}.hdf5',2*i)\n",
    "#     h5f = h5py.File(train_chunk_0_fname, 'w')\n",
    "#     h5f.create_dataset('images', data = train_chunk_images[:train_split_idx])\n",
    "#     h5f.create_dataset('labels', data = train_y_chuck[:train_split_idx])\n",
    "#     h5f.close()\n",
    "#     print('saved '+train_chunk_0_fname)\n",
    "\n",
    "#     train_chunk_1_fname = str.format('Data/bengaliai-cv19/train_image_data_processed{0}.hdf5',2*i+1)\n",
    "#     h5f = h5py.File(train_chunk_1_fname, 'w')\n",
    "#     h5f.create_dataset('images', data = train_chunk_images[train_split_idx:])\n",
    "#     h5f.create_dataset('labels', data = train_y_chuck[train_split_idx:])\n",
    "#     h5f.close()\n",
    "#     print('saved '+train_chunk_1_fname)\n",
    "\n",
    "\n",
    "#     ## for reading - \n",
    "#     # In [10]: h5f = h5py.File('data.h5','r')\n",
    "#     # In [11]: b = h5f['dataset_1'][:]\n",
    "#     # In [12]: h5f.close()\n",
    "\n",
    "#     chunk_valid_images = all_chunk_images[chunk_valid_indexes-50210*i] # valid\n",
    "#     chunk_valid_labels = y[chunk_valid_indexes]\n",
    "#     assert(chunk_valid_labels.shape[0]==chunk_valid_images.shape[0])\n",
    "#     valid_images.append(chunk_valid_images)\n",
    "#     valid_labels.append(chunk_valid_labels)\n",
    "\n",
    "#     chunk_test_images = all_chunk_images[chunk_test_indexes-50210*i] # test\n",
    "#     chunk_test_labels = y[chunk_test_indexes]\n",
    "#     assert(chunk_test_labels.shape[0]==chunk_test_images.shape[0])\n",
    "#     test_images.append(chunk_test_images)\n",
    "#     test_labels.append(chunk_test_labels)\n",
    "\n",
    "#     del train_chunk_images, train_y_chuck\n",
    "#     gc.collect()\n",
    "#     print('completed '+train_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving valid and test \n",
    "\n",
    "# valid_images_all = np.vstack(valid_images)\n",
    "# valid_labels_all = np.vstack(valid_labels)\n",
    "# test_images_all = np.vstack(test_images)\n",
    "# test_labels_all = np.vstack(test_labels)\n",
    "\n",
    "# ## Not dry, but can live with it, as it's one time thing\n",
    "# h5f = h5py.File('Data/bengaliai-cv19/valid_image_data_processed.hdf5', 'w')\n",
    "# h5f.create_dataset('images', data = valid_images_all)\n",
    "# h5f.create_dataset('labels', data = valid_labels_all)\n",
    "# h5f.close()\n",
    "\n",
    "# h5f = h5py.File('Data/bengaliai-cv19/test_image_data_processed.hdf5', 'w')\n",
    "# h5f.create_dataset('images', data = test_images_all)\n",
    "# h5f.create_dataset('labels', data = test_labels_all)\n",
    "# h5f.close()\n",
    "\n",
    "# del valid_images,valid_labels,test_images,test_labels\n",
    "# del valid_images_all,valid_labels_all,test_images_all,test_labels_all\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_generator(batch_size):\n",
    "    \n",
    "    files = [str.format('Data/bengaliai-cv19/train_image_data_processed{0}.hdf5',i) for i in range(8) ]\n",
    "    current_file_idx = 0\n",
    "    already_send = 0     \n",
    "    steps = 0\n",
    "    \n",
    "    while True:\n",
    "#     ## for reading - \n",
    "#     # In [10]: h5f = h5py.File('data.h5','r')\n",
    "#     # In [11]: b = h5f['dataset_1'][:]\n",
    "#     # In [12]: h5f.close()\n",
    "        h5f = h5py.File(files[current_file_idx],'r')\n",
    "        train_X = h5f['images'][:]\n",
    "        train_Y = h5f['labels'][:]        \n",
    "        h5f.close()        \n",
    "#         print(\"loaded \"+str(files[current_file_idx])+ \" with size \"+str(train_X.shape))        \n",
    "        \n",
    "        while (already_send<train_X.shape[0]):\n",
    "            if (already_send+batch_size<train_X.shape[0]):\n",
    "                train_batch_X = train_X[already_send:already_send+batch_size,:]\n",
    "                train_batch_Y = train_Y[already_send:already_send+batch_size,:]\n",
    "            else:\n",
    "                train_batch_X = train_X[already_send:,:]\n",
    "                train_batch_Y = train_Y[already_send:,:]\n",
    "            assert (train_batch_X.shape[0]==train_batch_Y.shape[0])                \n",
    "            steps+=1\n",
    "#             print(\"sending \"+str(already_send) + \" steps \" + str(steps))\n",
    "            \n",
    "            yield np.expand_dims(train_batch_X,axis=3), train_batch_Y\n",
    "            already_send+=batch_size\n",
    "\n",
    "#         print(\"file completed, swithing to next. Steps = \"+str(steps))\n",
    "        already_send=0\n",
    "        current_file_idx+=1\n",
    "        if (current_file_idx==8):\n",
    "            current_file_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_file_name = 'Data/bengaliai-cv19/valid_image_data_processed.hdf5'\n",
    "h5f = h5py.File(valid_file_name,'r')\n",
    "valid_X = np.expand_dims(h5f['images'][:],axis=3)\n",
    "valid_Y = h5f['labels'][:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    '''\n",
    "    Constructs the ML model used to predict handwritten digits.\n",
    "    Taken from https://github.com/tensorflow/models/blob/master/official/vision/image_classification/mnist_main.py\n",
    "    '''\n",
    "    image = tf.keras.layers.Input(shape=(137, 236, 1))\n",
    "    y = tf.keras.layers.Conv2D(filters=32,\n",
    "                             kernel_size=5,\n",
    "                             padding='same',\n",
    "                             activation='relu')(image)\n",
    "\n",
    "    y = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n",
    "                                   strides=(2, 2),\n",
    "                                   padding='same')(y)\n",
    "    \n",
    "    y = tf.keras.layers.Conv2D(filters=32,\n",
    "                             kernel_size=5,\n",
    "                             padding='same',\n",
    "                             activation='relu')(y)\n",
    "    \n",
    "    y = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n",
    "                                   strides=(2, 2),\n",
    "                                   padding='same')(y)\n",
    "    y = tf.keras.layers.Flatten()(y)\n",
    "    y = tf.keras.layers.Dense(1024, activation='relu')(y)\n",
    "    y = tf.keras.layers.Dropout(0.4)(y)\n",
    "    \n",
    "#     out_m = tf.keras.layers.Dense(186, activation='sigmoid')(y)\n",
    "    \n",
    "    out_0 = tf.keras.layers.Dense(11, activation='softmax')(y)\n",
    "    out_1 = tf.keras.layers.Dense(168, activation='softmax')(y)\n",
    "    out_2 = tf.keras.layers.Dense(7, activation='softmax')(y)\n",
    "    \n",
    "    out_m=tf.concat(values=[out_0,out_1,out_2],axis=1)\n",
    "        \n",
    "#         # vowel_diacritic\n",
    "#         self.fc1 = nn.Linear(512,11)\n",
    "#         # grapheme_root\n",
    "#         self.fc2 = nn.Linear(512,168)\n",
    "#         # consonant_diacritic\n",
    "#         self.fc3 = nn.Linear(512,7)\n",
    "\n",
    "#     probs = tf.keras.layers.Dense(10, activation='softmax')(y)\n",
    "\n",
    "#      Idea of multiclass classification from the link below - replace activation with sigmoid\n",
    "#    loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "#     https://www.pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/\n",
    "\n",
    "    model = tf.keras.models.Model(image, out_m, name='bengali')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#       np.expand_dims(X_train,axis=3),y_train,\n",
    "#       epochs=3,\n",
    "#       batch_size=64,\n",
    "#       validation_data=(np.expand_dims(X_test,axis=3),y_test)\n",
    "# #       steps_per_epoch=train_steps,\n",
    "# #       callbacks=callbacks,\n",
    "# #       validation_steps=num_eval_steps,\n",
    "# #       validation_data=eval_input_dataset,\n",
    "# #       validation_freq=flags_obj.epochs_between_evals\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def customLoss(yTrue,yPred):\n",
    "#     return keras.losses.categorical_crossentropy(yTrue[:,0:11],yPred[:,0:11])+\\\n",
    "#         2*keras.losses.categorical_crossentropy(yTrue[:,11:179],yPred[:,11:179])+\\\n",
    "#         keras.losses.categorical_crossentropy(yTrue[:,179:186],yPred[:,179:186])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/43076609/how-to-calculate-precision-and-recall-in-keras \n",
    "# # -> https://github.com/keras-team/keras/commit/a56b1a55182acf061b1eb2e2c86b48193a0e88f7\n",
    "\n",
    "# def recall(y_true, y_pred):\n",
    "#     \"\"\"Recall metric.\n",
    "#     Only computes a batch-wise average of recall.\n",
    "#     Computes the recall, a metric for multi-label classification of\n",
    "#     how many relevant items are selected.\n",
    "#     \"\"\"\n",
    "#     true_positives = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true * y_pred, 0, 1)))\n",
    "#     possible_positives = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true, 0, 1)))\n",
    "#     recall = true_positives / (possible_positives + keras.backend.epsilon())\n",
    "#     return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def customMetric(yTrue,yPred):\n",
    "# #     return keras.metrics.Recall(yTrue,yPred)\n",
    "#     return keras.metrics.Recall(yTrue[:,0:11],yPred[:,0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customMetric(valid_Y,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "#     loss=customLoss,\n",
    "    loss = keras.losses.binary_crossentropy,\n",
    "    metrics=['binary_accuracy',keras.metrics.Recall()])\n",
    "\n",
    "model_fname = 'models/model-' + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks reused from \n",
    "# https://www.kaggle.com/deshwalmahesh/bengali-ai-complete-beginner-tutorial-95-acc\n",
    "    \n",
    "import tensorflow.keras.callbacks as cb \n",
    "\n",
    "R_LR_P = cb.ReduceLROnPlateau(monitor='val_out_1_loss',patience=3,verbose=1,factor=0.5,min_lr=0.00001)\n",
    "# if validation loss of out_1 is not decreasing for 3 consecutive epochs, decrease the learning rate by 0.5 \n",
    "# given if learning rate is above 0.00001 and give us little insight on what has happened verbose=1\n",
    "\n",
    "ES = cb.EarlyStopping(monitor='val_loss',patience=4, min_delta=0.0025)\n",
    "# stop the model from fitting data if validation loss has not decreased by 0.0025 in the last 5 epochs\n",
    "\n",
    "MCP = cb.ModelCheckpoint(model_fname, monitor ='val_loss', verbose =1, \n",
    "                      save_best_only = True, save_weights_only=True)\n",
    "# save the weights in a file name specified only if the validation loss of out_1 layer has improved from \n",
    "# last save. out_1 because it's recall matters twice \n",
    "\n",
    "callbacks = [R_LR_P,ES,MCP]\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(model_fname+'.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      images_generator(batch_size=64),\n",
    "      epochs=12,\n",
    "      steps_per_epoch=3080,\n",
    "      validation_data = (valid_X, valid_Y),\n",
    "      callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "# plt.plot(history.history['recall'])\n",
    "# plt.plot(history.history['val_recall'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fname = 'models/model-2020-01-04-11-33'\n",
    "# model_fname = 'models/model-2020-01-09-06-03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate inference phase\n",
    "\n",
    "import tensorflow.keras.models\n",
    "# load json and create model\n",
    "json_file = open(model_fname+'.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(model_fname+'.hdf5')\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(optimizer=keras.optimizers.Adam(),\n",
    "#     loss=customLoss,\n",
    "    loss = keras.losses.binary_crossentropy,\n",
    "    metrics=['binary_accuracy',keras.metrics.Recall()])\n",
    "score = loaded_model.evaluate(valid_X, valid_Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_model.metrics_names)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric from rules\n",
    "```\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "scores = []\n",
    "for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n",
    "    y_true_subset = solution[solution[component] == component]['target'].values\n",
    "    y_pred_subset = submission[submission[component] == component]['target'].values\n",
    "    scores.append(sklearn.metrics.recall_score(\n",
    "        y_true_subset, y_pred_subset, average='macro'))\n",
    "final_score = np.average(scores, weights=[2,1,1])\n",
    "```\n",
    "also keep in mind target is constructed as\n",
    "```\n",
    "y_1 = pd.get_dummies(train_df.vowel_diacritic).values\n",
    "y_2 = pd.get_dummies(train_df.grapheme_root).values\n",
    "y_3 = pd.get_dummies(train_df.consonant_diacritic).values\n",
    "assert(y.shape[1]==11+168+7)\n",
    "del train_df, y_1,y_2,y_3\n",
    "```\n",
    "\n",
    "plan: \n",
    "1. get prediction on validation set\n",
    "2. try to compute metric for val set \n",
    "3. try to incorporate metric into training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=loaded_model.predict(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(valid_Y.shape==predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for intervals in [(0,11),(11,179),(179,186)]:    \n",
    "    y_true_subset = valid_Y[:,intervals[0]:intervals[1]]\n",
    "    y_pred_subset = predictions[:,intervals[0]:intervals[1]]\n",
    "    #\"hard\" max\n",
    "    y_pred_choice = np.zeros(y_pred_subset.shape, dtype='uint8')\n",
    "    y_pred_choice[np.arange(len(y_pred_subset)), y_pred_subset.argmax(axis=1)] = 1   \n",
    "    \n",
    "    partical_score = sklearn.metrics.recall_score(\n",
    "        y_true_subset, y_pred_choice, average='macro')\n",
    "    scores.append(partical_score)\n",
    "    \n",
    "    print('for '+str(intervals)+ ' score is '+str(partical_score))\n",
    "    \n",
    "\n",
    "final_score = np.average(scores, weights=[1,2,1])\n",
    "print()\n",
    "print('TOTAL: '+str(final_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcome\n",
    "\n",
    "| Model                       |file name | Local Recall   |LB score |  Ep |Comment|\n",
    "|-----------------------------|----------|----------------|---------|-----|-----------|\n",
    "| mnist_main (186 Dense out), custom loss  | model-2020-01-09-05-43 |0.0614  |  | 6  |Seems there is a bug because keras recall doesn't equal recall per model|\n",
    "| mnist_main (186 Dense out), binary_crossenthropy loss | model-2020-01-09-06-03 |0.5828 | 0.5723 | 6 | Custom loss makes training much worse|\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
