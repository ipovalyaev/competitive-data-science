{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import sklearn.linear_model\n",
    "from sklearn import preprocessing\n",
    "import xgboost\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../readonly/final_project_data/'\n",
    "\n",
    "transactions    = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv.gz'))\n",
    "items           = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\n",
    "item_categories = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))\n",
    "shops           = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\n",
    "\n",
    "test_set=pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset exploration \n",
    "based on Pandas basics notebook from week1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From week 1:\n",
    "# How many items are there, such that their price stays constant during the whole period of time?\n",
    "# Remember, the data can sometimes be noisy. :)\n",
    "# grouping3=pd.DataFrame(transactions[['item_id','item_price']],copy=True)\n",
    "# grouping3['item_price']=abs(grouping3['item_price']).round(decimals=2)\n",
    "# grouping3=grouping3.drop_duplicates()\n",
    "# sum3=grouping3.groupby('item_id').count()\n",
    "# sum3[sum3['item_price']==1].count()\n",
    "\n",
    "# Maybe could be the feature (price changes count, last price delta, price changed time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA - to do :\n",
    " 1. analyze time dependancies if any for different intervals\n",
    " 2. plot some dependancies?\n",
    " 3. From the task: Target distribution is visualized, time trend is assessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing simple submittion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A good exercise is to reproduce previous_value_benchmark.</b>\n",
    "\n",
    "<i> Stolen from somewhere from course materials </i>\n",
    "\n",
    "\n",
    "As the name suggest - in this benchmark for the each shop/item pair our predictions are just monthly sales from the previous month, i.e. October 2015.\n",
    "\n",
    "The most important step at reproducing this score is correctly aggregating daily data and constructing monthly sales data frame. You need to get lagged values, fill NaNs with zeros and clip the values into [0,20] range. If you do it correctly, you'll get precisely 1.16777 on the public leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filter=transactions.set_index(pd.to_datetime(transactions[\"date\"],format='%d.%m.%Y'))[\"2015-10\"]\n",
    "# df_october=pd.DataFrame(df_filter)\n",
    "\n",
    "# df_october_groupped = df_october.groupby(['shop_id','item_id'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "\n",
    "# predictions = pd.merge(test_set,df_october_groupped,\n",
    "#                        left_on=('shop_id','item_id'), right_on=('shop_id','item_id'), how='left')\n",
    "# ##filling NAs\n",
    "# predictions.item_cnt_day=predictions.item_cnt_day.fillna(0.0)\n",
    "# ## clipping\n",
    "# predictions.loc[predictions.item_cnt_day>20,'item_cnt_day']=20\n",
    "# predictions.loc[predictions.item_cnt_day<0,'item_cnt_day']=0\n",
    "# ##dropping extra colums and indexes after merge\n",
    "# predictions.drop(labels=['shop_id','item_id'],axis=1,inplace=True)\n",
    "# predictions.rename(columns={'item_cnt_day':'item_cnt_month'},inplace=True)\n",
    "\n",
    "# # converting  ID back to int\n",
    "# predictions.ID = predictions.ID.astype(int)\n",
    "\n",
    "# filename = 'submition_{}.csv'.format(time.strftime('%Y_%m_%d_%H_%M',time.localtime()))\n",
    "# filename = os.path.join(DATA_FOLDER,filename)\n",
    "# predictions.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split investigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_test = test_set.item_id.nunique()\n",
    "shops_test = test_set.shop_id.nunique()\n",
    "\n",
    "items_train = transactions.item_id.unique().shape[0]\n",
    "shops_train = transactions.shop_id.unique().shape[0]\n",
    "\n",
    "print ('Test set: \\n'+\\\n",
    "       ' items: ' + str(items_test) + ' shops: ' + str(shops_test) + \\\n",
    "       '\\n total: '+ str(test_set[\"ID\"].count()) + ' supposed total: '+str(items_test*shops_test) + \\\n",
    "       '\\nData set: \\n'+\\\n",
    "       ' items: ' + str(items_train) + ' shops: ' + str(shops_train) + \\\n",
    "       '\\n total: ' + str (items_train*shops_train)\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the cartesian product is used in a test set, however only some of shops and items included into test set.  This suggests potential dataleak, but further investigation needed.\n",
    "also hypotesis to test - shops are beeing closed, as well items are get offsale. Maybe data missing in testset is just one which are not exists anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_shops = np.setxor1d(transactions.shop_id.unique(),test_set.shop_id.unique())\n",
    "items_train_only = transactions[~transactions.item_id.isin(test_set.item_id)].item_id.unique()\n",
    "\n",
    "print ('excluded_shops ' + str(excluded_shops))\n",
    "print ('items only in train count ' + str(items_train_only.shape[0]))\n",
    "\n",
    "# There is also items only in test set, but let's ignore it for now\n",
    "print ('items only in test count ' + str(test_set[~test_set.item_id.isin(transactions.item_id.unique())].\\\n",
    "                                         item_id.unique().shape[0]))\n",
    "print ('average date_block for: \\n'+\\\n",
    "       '  exluded shops: ' + \\\n",
    "        str(transactions[transactions.shop_id.isin(excluded_shops)].date_block_num.mean()) + \\\n",
    "       '\\n  excluded items: ' + \\\n",
    "        str(transactions[transactions.item_id.isin(items_train_only)].date_block_num.mean()) + \\\n",
    "        '\\n  average for all data : ' + \\\n",
    "        str(transactions.date_block_num.mean()) + \\\n",
    "       '\\nlast seen:' + \\\n",
    "       '\\n  exluded shops: ' + \\\n",
    "       str(transactions[transactions.shop_id.isin(excluded_shops)]. \\\n",
    "           groupby('shop_id'). \\\n",
    "           agg({'date_block_num':'max'}).mean().values[0]) +\\\n",
    "       '\\n  excluded items: ' + \\\n",
    "       str(transactions[transactions.item_id.isin(items_train_only)]. \\\n",
    "           groupby('item_id'). \\\n",
    "           agg({'date_block_num':'max'}).mean().values[0]) +\\\n",
    "       '\\n  average for all shops : ' + \\\n",
    "       str(transactions.groupby('shop_id').agg({'date_block_num':'max'}).mean().values[0]) +\\\n",
    "       '\\n  average for all items : ' + \\\n",
    "       str(transactions.groupby('item_id').agg({'date_block_num':'max'}).mean().values[0]) \n",
    "      )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems average date is less for absent shops, so we can somehow conlude the shops not in test set are closed. \n",
    "However, it's not that clear for items, why only about 1/4 of items are in test set. \n",
    "\n",
    "It also doesn't looks like there are only items which was sold, as leaderboard probing shown mean 0.2839\n",
    "https://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/79142\n",
    "\n",
    "There could be a case items excluded are the one's had 0 sales in all the possible shops within the given month (which also suggested by train/test split in week4). The last one can be tested by grouping transactions by date_block_num and count unique items over to see if it's close to test set size\n",
    "\n",
    "This can be utilized together with mean information above during predicting for items not in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.groupby('date_block_num').agg({'item_id':'nunique'}).mean().values[0]\n",
    "\n",
    "## Seems pretty close (espesially if look and number's not mean - 5100 looks reasonable values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "Models are defines in this section to be available to validations (while beeing developed later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_model(train_set, test_set,fit_col):\n",
    "    # we can take global mean of training data, but it's not so easy to calculate, \n",
    "    # as we need to produce proper fraction of zeros. So just constant for some period taken\n",
    "    return np.ones((test_set.shape[0],1),dtype='float32')*0.298 ## true_values['item_cnt_day'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_value_model(train_set, test_set,fit_col):\n",
    "    max_date_block = train_set.date_block_num.max()\n",
    "    \n",
    "    predictions = test_set.merge(train_set[train_set.date_block_num == max_date_block],\\\n",
    "        how='left',on=['shop_id','item_id']).fillna(0.0)\n",
    "    \n",
    "    predictions.loc[predictions.target>20,'target']=20\n",
    "    predictions.loc[predictions.target<0,'target']=0\n",
    "    \n",
    "    return predictions['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_model (train_set, test_set,fit_col):\n",
    "    lr = sklearn.linear_model.LinearRegression(n_jobs=4)\n",
    "    X = train_set[fit_col]\n",
    "    y = train_set['target']\n",
    "    lr.fit(X.values,y.values)\n",
    "    predictions = lr.predict(test_set[fit_col].values)\n",
    "\n",
    "    return predictions.clip(min=0.0, max =20.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_model_ellastic (train_set, test_set,fit_col):\n",
    "    lr = sklearn.linear_model.ElasticNet()\n",
    "    X = train_set[fit_col]\n",
    "    y = train_set['target']\n",
    "    lr.fit(X.values,y.values)\n",
    "    predictions = lr.predict(test_set[fit_col].values)    \n",
    "\n",
    "    return predictions.clip(min=0.0, max =20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_model_ellastic_with_feature_norm (train_set, test_set,fit_col):\n",
    "    ## Seems with MinMaxScaler exactly the same metrics as lin_model. Just, probably given by the \n",
    "    ## fact feature are centered in LinearRegression and not too far from 0-1\n",
    "    ## \n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "    reduced_X_train = pd.DataFrame(train_set[fit_col+['target']],copy=True)\n",
    "    X_train_minmax = scaler.fit_transform(reduced_X_train[fit_col])\n",
    "    reduced_X_train[fit_col] = X_train_minmax\n",
    "\n",
    "    reduced_X_test = pd.DataFrame(test_set[fit_col],copy=True)\n",
    "    X_test_minmax = scaler.transform(reduced_X_test)\n",
    "    reduced_X_test[fit_col] = X_test_minmax\n",
    "\n",
    "    predictions=lin_model_ellastic(reduced_X_train,reduced_X_test,fit_col)\n",
    "    \n",
    "    del reduced_X_train, reduced_X_test, X_train_minmax, X_test_minmax\n",
    "    gc.collect();\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_model(train_set, test_set,fit_col):\n",
    "    xgb_model = xgboost.XGBRegressor(n_jobs=4,objective='reg:squarederror')\n",
    "    xgb_model.fit(train_set[fit_col].values,train_set['target'].values)\n",
    "    predictions = xgb_model.predict(test_set[fit_col].values)\n",
    "    \n",
    "    return predictions.clip(min=0.0, max =20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_model(train_set, test_set,fit_col):\n",
    "    lgb_params = {\n",
    "#                'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':4, \n",
    "#                'min_data_in_leaf': 2**7, \n",
    "#                'bagging_fraction': 0.75, \n",
    "#                'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "#                'bagging_seed': 2**7, \n",
    "#                'num_leaves': 2**7,\n",
    "#                'bagging_freq':1,\n",
    "#                'verbose':0 \n",
    "              }\n",
    "\n",
    "    model = lgb.train(lgb_params, lgb.Dataset(train_set[fit_col].values, label=train_set['target'].values), 100)\n",
    "    predictions = model.predict(test_set[fit_col].values)\n",
    "    return predictions.clip(min=0.0, max =20.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_product(*arrays):\n",
    "    la = len(arrays)\n",
    "    dtype = np.result_type(*arrays)\n",
    "    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n",
    "    for i, a in enumerate(np.ix_(*arrays)):\n",
    "        arr[...,i] = a\n",
    "    return arr.reshape(-1, la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_transactions (transactions):\n",
    "    '''\n",
    "    For each dataframe select unique shops and unique items, make all possible pairs out of it \n",
    "    and for each sych a pair either aggregate transactions to get sales, or put 0 if no transactions.\n",
    "    The logic was implemented in week4 for validation, also it's the way, how test set is constructed\n",
    "    '''\n",
    "    index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "    grid = [] \n",
    "    for block_num in transactions['date_block_num'].unique():\n",
    "        cur_shops = transactions.loc[transactions['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "        cur_items = transactions.loc[transactions['date_block_num'] == block_num, 'item_id'].unique()\n",
    "        # https://docs.python.org/3/library/itertools.html#itertools.product\n",
    "        # Cartesian product of input iterables.\n",
    "        grid.append(cartesian_product(cur_shops, cur_items, np.array([block_num])))\n",
    "    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)    \n",
    "\n",
    "    gb = transactions.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    gb.rename(columns={'item_cnt_day':'target'},inplace=True)\n",
    "    return pd.merge(grid, gb, how='left', on=index_cols).fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_shifts(trainset, testset):\n",
    "    '''\n",
    "    !!!\n",
    "    If this transformation is called - it should be called immidiately after groupping, \n",
    "    otherwise all additional features will be lost\n",
    "    !!!\n",
    "    \n",
    "    Generate for both train and testset target, shifted by some month as a feature.\n",
    "    testset assumed one date_block_num ahead of train.\n",
    "    '''\n",
    "    shift_range = [1, 2, 3, 4, 5, 12]\n",
    "    \n",
    "    max_block_num = trainset.date_block_num.max()\n",
    "    fit_cols = []\n",
    "    testset.loc[:,'date_block_num']=max_block_num+1\n",
    "    \n",
    "    ## groups by shop and item\n",
    "    trainset = trainset.merge(\n",
    "        trainset.groupby(['shop_id','date_block_num'],as_index=False).\\\n",
    "            agg({'target':'sum'}).\\\n",
    "            rename(columns={'target':'target_shop'}),\n",
    "        on=['shop_id','date_block_num'], copy=False, validate = 'm:1').fillna(0.0)\n",
    "    trainset = trainset.merge(\n",
    "        trainset.groupby(['item_id','date_block_num'],as_index=False).\\\n",
    "            agg({'target':'sum'}).\\\n",
    "            rename(columns={'target':'target_item'}),\n",
    "        on=['item_id','date_block_num'], copy=False, validate = 'm:1').fillna(0.0)\n",
    "        \n",
    "    ## generating shifts\n",
    "    for shift in shift_range:\n",
    "        curr_fit_cols = ['target_lag_{}'.format(shift),'target_shop_lag_{}'.format(shift),'target_item_lag_{}'.format(shift)]\n",
    "        ##\n",
    "        trainset.loc[:,'shifted_date_block_num'] = trainset['date_block_num']+shift\n",
    "\n",
    "        testset=\\\n",
    "            testset.merge(trainset[['shop_id', 'item_id', 'shifted_date_block_num', 'target','target_shop','target_item']],\n",
    "                      left_on= ['shop_id', 'item_id', 'date_block_num'],\n",
    "                      right_on=['shop_id', 'item_id', 'shifted_date_block_num'], how='left',\n",
    "                      validate='1:1',copy=False,\n",
    "            )[['shop_id', 'item_id', 'date_block_num', 'target','target_shop','target_item']+fit_cols].\\\n",
    "            copy().fillna(0).rename(columns={'target':curr_fit_cols[0],\n",
    "                                      'target_shop':curr_fit_cols[1],\n",
    "                                      'target_item':curr_fit_cols[2]})\n",
    "\n",
    "        trainset=\\\n",
    "            trainset[['shop_id', 'item_id', 'date_block_num', 'target','target_shop','target_item']+fit_cols].merge(\n",
    "            trainset[['shop_id', 'item_id', 'shifted_date_block_num', 'target','target_shop','target_item']],\n",
    "                       left_on= ['shop_id', 'item_id', 'date_block_num'], \n",
    "                        right_on=['shop_id', 'item_id', 'shifted_date_block_num'], how='left',\n",
    "                        suffixes = ('','_shift'), validate='1:1',copy=False,\n",
    "            )[['shop_id', 'item_id', 'date_block_num', 'target','target_shift',\n",
    "               'target_shop','target_item','target_shop_shift','target_item_shift']+fit_cols].\\\n",
    "            copy().fillna(0).rename(columns={'target_shift':curr_fit_cols[0],\n",
    "                                      'target_shop_shift':curr_fit_cols[1],\n",
    "                                      'target_item_shift':curr_fit_cols[2]})\n",
    "        \n",
    "        fit_cols.extend(curr_fit_cols)\n",
    "    gc.collect()\n",
    "    return (trainset,testset,fit_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_shifted_features(trainset, testset):\n",
    "    \n",
    "#     '''\n",
    "#     added features for sale in same shop, then for same item and then for pair (shop, item) for 1,2,3,4,6,9,12 month ago\n",
    "#     in : trainset - pandas dataframe with train data (transactions), testset - shop_id, item_id pairs for predictions\n",
    "#     out - tuple (trainset, testset), where trainset is groupped by shop_id, item_id either\n",
    "    \n",
    "#     '''\n",
    "#     index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "#     shift_range = [1, 2, 3, 4, 5, 12]\n",
    "#     # shift_range = [1, 2]\n",
    "\n",
    "#     grid = [] \n",
    "#     for block_num in trainset['date_block_num'].unique():\n",
    "#         cur_shops = trainset.loc[trainset['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "#         cur_items = trainset.loc[trainset['date_block_num'] == block_num, 'item_id'].unique()\n",
    "#         # https://docs.python.org/3/library/itertools.html#itertools.product\n",
    "#         # Cartesian product of input iterables.\n",
    "#         grid.append(cartesian_product(cur_shops, cur_items, np.array([block_num])))\n",
    "#     # Turn the grid into a dataframe\n",
    "#     grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "#     gb = trainset.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\n",
    "#     gb.rename(columns={'item_cnt_day':'target'},inplace=True)\n",
    "#     # Join it to the grid\n",
    "#     all_data = pd.merge(grid, gb, how='left', on=['shop_id','item_id','date_block_num']).fillna(0)\n",
    "\n",
    "#     # Same as above but with shop-month aggregates\n",
    "#     gb = trainset.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "#     gb.rename(columns={'item_cnt_day':'target_shop'},inplace=True)\n",
    "#     all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "#     # Same as above but with item-month aggregates\n",
    "#     gb = trainset.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "#     gb.rename(columns={'item_cnt_day':'target_item'},inplace=True)\n",
    "#     all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "    \n",
    "#     # List of columns that we will use to create lags\n",
    "#     cols_to_rename = list(all_data.columns.difference(index_cols))\n",
    "    \n",
    "#     max_block_num = all_data.date_block_num.max()\n",
    "#     testset['date_block_num']=max_block_num+1\n",
    "#     # shifted features are also done in kinda weird fashion - probably there is easier way \n",
    "#     for month_shift in shift_range:\n",
    "#         train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "#         train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "#         foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "#         train_shift = train_shift.rename(columns=foo) #rename using lambda.Doesn't seems most transparent, but ok\n",
    "#         train_shift.head()        \n",
    "#         testset = pd.merge(testset, train_shift, on=index_cols, how='left').fillna(0)\n",
    "#         all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "        \n",
    "#     del train_shift\n",
    "\n",
    "#     # Don't use old data which have 0 shift and will just confuse the model\n",
    "#     all_data = all_data[all_data['date_block_num'] >= 12]\n",
    "\n",
    "#     # List of all lagged features\n",
    "#     # this will cause a problem, if 1 is excluded and 10 included, for ex. But probably this shouldn't happen\n",
    "#     fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "#     # We will drop these at fitting stage\n",
    "#     to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols)) - set (['target'])) \n",
    "#     all_data.drop(to_drop_cols,axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "#     del grid, gb \n",
    "#     gc.collect();\n",
    "    \n",
    "#     return (all_data,testset,fit_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Before playing with any kind of models validation has to be setup. \n",
    "Some ideas could be taken from week4. \n",
    "\n",
    "In contrast with it 4 cycles of validation to be used - train on date_block_num [0:i] -> validate on [i+1], where i [30,31,32,33]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume every model should take dataset like transations and test pairs (like test_set) and return dataset of prediction \n",
    "[shop_id, item_id, prediction] for the month, next to the last present in dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_block = 30\n",
    "to_block = 33\n",
    "\n",
    "def validate (models):\n",
    "    outcome = []\n",
    "    for i in range(from_block,to_block+1):    \n",
    "        print ('validation: generating test and train for i='+str(i))\n",
    "        trainset = group_transactions(transactions[transactions.date_block_num<i])\n",
    "        testset = group_transactions(transactions[transactions.date_block_num==i])\n",
    "        \n",
    "        (train_groupped_with_shifted, test_set_with_shifted, fit_col) = \\\n",
    "            make_shifts(trainset,testset[['shop_id','item_id']].copy())## copy to avoid SettingWithCopyWarning\n",
    "\n",
    "        for model in models:\n",
    "            print ('validation: testing model ' + model.__name__)\n",
    "            predictions = model(train_groupped_with_shifted, test_set_with_shifted,fit_col)\n",
    "\n",
    "            # According to competition description validation is done with clipping, so we should do the same\n",
    "            # Model is responsible for it's own clipping if nessesary\n",
    "            # r2 should be as close to 1 as possible. 0 is constant model            \n",
    "            r2s = r2_score(testset['target'].clip(lower=0.0, upper =20.0), predictions)\n",
    "            rmse = np.sqrt(mean_squared_error(testset['target'].clip(lower=0.0, upper =20.0), predictions))\n",
    "            print('rmse = '+str(rmse))\n",
    "            outcome.append({'model':model.__name__,'block':i,'metric':'r2s','value':r2s})\n",
    "            outcome.append({'model':model.__name__,'block':i,'metric':'rmse','value':rmse})\n",
    "                \n",
    "        print () \n",
    "        \n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# resut = validate([constant_model,previous_value_model,lin_model,xgboost_model])\n",
    "# resut = validate([constant_model,previous_value_model,lin_model, lin_model_with_feature_norm, lightgbm_model])\n",
    "resut = validate(\n",
    "    [previous_value_model,lin_model, lin_model_ellastic,lin_model_ellastic_with_feature_norm]\n",
    "    )\n",
    "\n",
    "pd.DataFrame(resut).groupby(['model','metric'],as_index=False).agg({'value':'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation:\n",
    "\n",
    "|model| \tmetric \t|value|\n",
    "|----|-----|-----------|\n",
    "|constant_model| r2s|-0.000926|\n",
    "|constant_model|rmse|1.086481|\n",
    "|lin_model|r2s|0.307244|\n",
    "|lin_model|rmse|0.905361|\n",
    "|previous_value_model|r2s|0.138526|\n",
    "|previous_value_model|rmse|1.008575|\n",
    "|xgboost_model|r2s|0.358721|\n",
    "|xgboost_model|rmse|0.870930|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Seems local validation somehow corelates with public leaderboard </b>\n",
    "\n",
    "|Name|Score|Local score|Model|Comment|\n",
    "|----|-----|-----------|-----|-------|\n",
    "|submition_2019_06_03_09_29.csv.gz|1.16777|1.008575|Last month baseline||\n",
    "|submition_lin_model_2019_12_02_13_50.csv.gz|1.96182||simple linear model with 2 month shifts and with clipping 0-300|wrong cliipng|\n",
    "|submition_lin_model_2019_12_02_13_56.csv.gz|4.34576||simple linear model with 2 month shifts with clipping removed|no clipping|\n",
    "|submition_lin_model_2019_12_02_15_39.csv.gz|1.08014|0.905361|simple linear model, more with proper clipping|proper clipping apllied|\n",
    "|submition_xgboost_model_2019_12_03_15_43.csv.gz|1.02016|0.870930|simple xgboost without any parameters tuned|\n",
    "|submition_lightgbm_model_2019_12_04_05_53.csv.gz|**1.00715**|0.862635|simple light gbm without any parameters tuned|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_submit(model):\n",
    "    \n",
    "    trainset = group_transactions(transactions) # <-using global var in func \n",
    "    \n",
    "    (train_groupped_with_shifted, test_set_with_shifted, fit_col) = \\\n",
    "            make_shifts(trainset,test_set) # <-using global var in func \n",
    "    \n",
    "    predictions = model(train_groupped_with_shifted, test_set_with_shifted,fit_col)\n",
    "    \n",
    "    test_set_with_shifted.loc[:,'predictions'] = predictions.astype(float)\n",
    "    test_set_with_shifted.loc[:,'ID'] = test_set.ID.astype(int)        \n",
    "    test_set_with_shifted.rename(columns={'predictions':'item_cnt_month'},inplace=True)\n",
    "    \n",
    "\n",
    "    filename = 'submition_{}_{}.csv.gz'.format(model.__name__,time.strftime('%Y_%m_%d_%H_%M',time.localtime()))\n",
    "    test_set_with_shifted[['ID','item_cnt_month']].to_csv(filename,index=False)\n",
    "    print (filename + ' prepared')\n",
    "    \n",
    "    \n",
    "# prepare_submit(lightgbm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: TODO\n",
    "1. From the task - Type of public/private split is identified\n",
    "2. Compare for more complex model which metric better correlates with lederboard - rmse or r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # out of functions code to test model\n",
    "\n",
    "train_transactions = transactions[transactions.date_block_num<32]\n",
    "test_transactions = transactions[transactions.date_block_num==33]\n",
    "\n",
    "test_set = pd.DataFrame(cartesian_product(test_transactions.shop_id.unique(), \n",
    "                               test_transactions.item_id.unique()),columns=['shop_id','item_id'])\n",
    "\n",
    "true_values = test_set.merge ( \\\n",
    "    test_transactions.groupby(['shop_id','item_id'],as_index=False).agg({'item_cnt_day':'sum'}), \\\n",
    "    how = 'left' ).fillna(0)\n",
    "\n",
    "(train_groupped_with_shifted, test_set_with_shifted, fit_col) = \\\n",
    "    generate_shifted_features(train_transactions,true_values[['shop_id','item_id']])\n",
    "\n",
    "train_set = train_groupped_with_shifted\n",
    "test_set = test_set_with_shifted\n",
    "\n",
    "# # predictions = lin_model(train_set, test_set,fit_col)\n",
    "# # rmse = np.sqrt(mean_squared_error(true_values['item_cnt_day'].clip(lower=0.0, upper =20.0).values, predictions))\n",
    "# # print (rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea for features:\n",
    "# 1.1. word count in items and categories\n",
    "# 1.2. number of similar items by description\n",
    "\n",
    "# 1.3. TFIDF -> SVD https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py\n",
    "#  --> adding 150 SVD features from items description increases memory footprint, while rmse even worse\n",
    "#  --> 10 SVD features makes it more feasible, while, it doesn't change RMSE for linear model and makes it worse for \n",
    "# lightgbm\n",
    "# --> however, while doing clustering on top of SVD, seems lightgbm slightly benefits from it \n",
    "# in this scenario number of SVD components and number of clusters to be turned\n",
    "\n",
    "# 1.4. cluster text (item+category) and then cluster number\n",
    "\n",
    "# 2.1. count of items in given category\n",
    "# 2.2. item price (maybe also price changes)\n",
    "# 2.3. average category price\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add SVD features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer # the best by svd.explained_variance_ratio_.sum()\n",
    "from sklearn.feature_extraction.text import HashingVectorizer # almost as good as CountVectorizer but sdv runs infinite \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "vectorizer = HashingVectorizer()\n",
    "item_names_tfidf = vectorizer.fit_transform(items.item_name)\n",
    "svd_components_count = 10\n",
    "\n",
    "svd = TruncatedSVD(svd_components_count)\n",
    "item_names_svd = svd.fit_transform(item_names_tfidf)\n",
    "print('SDV explained_var ' + str(svd.explained_variance_ratio_.sum()))\n",
    "\n",
    "SVD_feature_names = {}\n",
    "for i in range(svd_components_count):\n",
    "    SVD_feature_names[i]='SVD_'+str(i)\n",
    "\n",
    "item_names_svd_pd = pd.DataFrame(item_names_svd)\n",
    "item_names_svd_pd.rename(columns=SVD_feature_names,inplace=True)\n",
    "items_with_svd = pd.concat([items, item_names_svd_pd],axis=1)\n",
    "\n",
    "train_set_updated = train_set.merge(items_with_svd, on = 'item_id', how = 'left')\n",
    "test_set_updated = test_set.merge(items_with_svd, on = 'item_id', how = 'left')\n",
    "\n",
    "del train_set, test_set\n",
    "\n",
    "_=[fit_col.append(x) for x in list(SVD_feature_names.values())]\n",
    "train_set = train_set_updated\n",
    "test_set = test_set_updated\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "item_names_tfidf = vectorizer.fit_transform(items.item_name)\n",
    "svd_components_count = 500\n",
    "svd = TruncatedSVD(svd_components_count)\n",
    "item_names_svd = svd.fit_transform(item_names_tfidf)\n",
    "print('SDV explained_var ' + str(svd.explained_variance_ratio_.sum()))\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=50, init='k-means++', max_iter=100, n_init=3)\n",
    "km.fit(item_names_svd)\n",
    "labels_feature = pd.DataFrame(km.labels_,columns=['km_item'])\n",
    "items_with_km = pd.concat([items, labels_feature],axis=1)\n",
    "train_set_updated = train_set.merge(items_with_km, on = 'item_id', how = 'left')\n",
    "test_set_updated = test_set.merge(items_with_km, on = 'item_id', how = 'left')\n",
    "fit_col.append('km_item')\n",
    "train_set = train_set_updated\n",
    "test_set = test_set_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_col.append('km_item')\n",
    "predictions = lightgbm_model(train_set, test_set,fit_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(true_values['item_cnt_day'].clip(lower=0.0, upper =20.0).values, predictions))\n",
    "print (rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=[fit_col.append(x) for x in list(SVD_feature_names.values())]\n",
    "predictions = lightgbm_model(train_set, test_set,fit_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(true_values['item_cnt_day'].clip(lower=0.0, upper =20.0).values, predictions))\n",
    "print (rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_col = ['target_lag_1','target_item_lag_1', 'target_shop_lag_1', 'target_lag_2', 'target_item_lag_2',\n",
    " 'target_shop_lag_2', 'target_lag_3', 'target_item_lag_3', 'target_shop_lag_3', 'target_lag_4',\n",
    " 'target_item_lag_4', 'target_shop_lag_4', 'target_lag_5', 'target_item_lag_5', 'target_shop_lag_5',\n",
    " 'target_lag_12', 'target_item_lag_12', 'target_shop_lag_12']\n",
    "\n",
    "predictions = lightgbm_model(train_set, test_set,fit_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(true_values['item_cnt_day'].clip(lower=0.0, upper =20.0).values, predictions))\n",
    "print (rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# xgb_model = xgboost.XGBRegressor(n_jobs=4,objective='reg:squarederror',verbosity=2)\n",
    "# xgb_model.fit(train_groupped_with_shifted[fit_col],train_groupped_with_shifted['target'])\n",
    "# predictions = xgb_model.predict(test_set_with_shifted[fit_col])\n",
    "# rmse = np.sqrt(mean_squared_error(true_values['item_cnt_day'].clip(lower=0.0, upper =20.0).values, predictions.clip(min=0.0, max =20.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "hw_version": "1.0.0",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
