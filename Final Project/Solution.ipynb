{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import sklearn.linear_model\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import xgboost\n",
    "import lightgbm as lgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../readonly/final_project_data/'\n",
    "\n",
    "transactions    = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv.gz'))\n",
    "items           = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\n",
    "item_categories = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))\n",
    "shops           = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\n",
    "\n",
    "test_set=pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset exploration \n",
    "based on Pandas basics notebook from week1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From week 1:\n",
    "# How many items are there, such that their price stays constant during the whole period of time?\n",
    "# Remember, the data can sometimes be noisy. :)\n",
    "# grouping3=pd.DataFrame(transactions[['item_id','item_price']],copy=True)\n",
    "# grouping3['item_price']=abs(grouping3['item_price']).round(decimals=2)\n",
    "# grouping3=grouping3.drop_duplicates()\n",
    "# sum3=grouping3.groupby('item_id').count()\n",
    "# sum3[sum3['item_price']==1].count()\n",
    "\n",
    "# Maybe could be the feature (price changes count, last price delta, price changed time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks weird, but there are only 2 such items anyway\n",
    "# items[items.item_name.str.startswith('!')==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA - to do :\n",
    " 1. analyze time dependancies if any for different intervals\n",
    " 2. plot some dependancies?\n",
    " 3. From the task: Target distribution is visualized, time trend is assessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing simple submittion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A good exercise is to reproduce previous_value_benchmark.</b>\n",
    "\n",
    "<i> Stolen from somewhere from course materials </i>\n",
    "\n",
    "\n",
    "As the name suggest - in this benchmark for the each shop/item pair our predictions are just monthly sales from the previous month, i.e. October 2015.\n",
    "\n",
    "The most important step at reproducing this score is correctly aggregating daily data and constructing monthly sales data frame. You need to get lagged values, fill NaNs with zeros and clip the values into [0,20] range. If you do it correctly, you'll get precisely 1.16777 on the public leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filter=transactions.set_index(pd.to_datetime(transactions[\"date\"],format='%d.%m.%Y'))[\"2015-10\"]\n",
    "# df_october=pd.DataFrame(df_filter)\n",
    "\n",
    "# df_october_groupped = df_october.groupby(['shop_id','item_id'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "\n",
    "# predictions = pd.merge(test_set,df_october_groupped,\n",
    "#                        left_on=('shop_id','item_id'), right_on=('shop_id','item_id'), how='left')\n",
    "# ##filling NAs\n",
    "# predictions.item_cnt_day=predictions.item_cnt_day.fillna(0.0)\n",
    "# ## clipping\n",
    "# predictions.loc[predictions.item_cnt_day>20,'item_cnt_day']=20\n",
    "# predictions.loc[predictions.item_cnt_day<0,'item_cnt_day']=0\n",
    "# ##dropping extra colums and indexes after merge\n",
    "# predictions.drop(labels=['shop_id','item_id'],axis=1,inplace=True)\n",
    "# predictions.rename(columns={'item_cnt_day':'item_cnt_month'},inplace=True)\n",
    "\n",
    "# # converting  ID back to int\n",
    "# predictions.ID = predictions.ID.astype(int)\n",
    "\n",
    "# filename = 'submition_{}.csv'.format(time.strftime('%Y_%m_%d_%H_%M',time.localtime()))\n",
    "# filename = os.path.join(DATA_FOLDER,filename)\n",
    "# predictions.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split investigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: \n",
      " items: 5100 shops: 42\n",
      " total: 214200 supposed total: 214200\n",
      "Data set: \n",
      " items: 21807 shops: 60\n",
      " total: 1308420\n"
     ]
    }
   ],
   "source": [
    "items_test = test_set.item_id.nunique()\n",
    "shops_test = test_set.shop_id.nunique()\n",
    "\n",
    "items_train = transactions.item_id.unique().shape[0]\n",
    "shops_train = transactions.shop_id.unique().shape[0]\n",
    "\n",
    "print ('Test set: \\n'+\\\n",
    "       ' items: ' + str(items_test) + ' shops: ' + str(shops_test) + \\\n",
    "       '\\n total: '+ str(test_set[\"ID\"].count()) + ' supposed total: '+str(items_test*shops_test) + \\\n",
    "       '\\nData set: \\n'+\\\n",
    "       ' items: ' + str(items_train) + ' shops: ' + str(shops_train) + \\\n",
    "       '\\n total: ' + str (items_train*shops_train)\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the cartesian product is used in a test set, however only some of shops and items included into test set.  This suggests potential dataleak, but further investigation needed.\n",
    "also hypotesis to test - shops are beeing closed, as well items are get offsale. Maybe data missing in testset is just one which are not exists anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluded_shops [ 0  1  8  9 11 13 17 20 23 27 29 30 32 33 40 43 51 54]\n",
      "items only in train count 17070\n",
      "items only in test count 363\n",
      "average date_block for: \n",
      "  exluded shops: 11.962617895419658\n",
      "  excluded items: 10.468482249471924\n",
      "  average for all data : 14.56991146343017\n",
      "last seen:\n",
      "  exluded shops: 20.38888888888889\n",
      "  excluded items: 18.841769185705918\n",
      "  average for all shops : 29.216666666666665\n",
      "  average for all items : 21.760489750997387\n"
     ]
    }
   ],
   "source": [
    "excluded_shops = np.setxor1d(transactions.shop_id.unique(),test_set.shop_id.unique())\n",
    "items_train_only = transactions[~transactions.item_id.isin(test_set.item_id)].item_id.unique()\n",
    "\n",
    "print ('excluded_shops ' + str(excluded_shops))\n",
    "print ('items only in train count ' + str(items_train_only.shape[0]))\n",
    "\n",
    "# There is also items only in test set, but let's ignore it for now\n",
    "print ('items only in test count ' + str(test_set[~test_set.item_id.isin(transactions.item_id.unique())].\\\n",
    "                                         item_id.unique().shape[0]))\n",
    "print ('average date_block for: \\n'+\\\n",
    "       '  exluded shops: ' + \\\n",
    "        str(transactions[transactions.shop_id.isin(excluded_shops)].date_block_num.mean()) + \\\n",
    "       '\\n  excluded items: ' + \\\n",
    "        str(transactions[transactions.item_id.isin(items_train_only)].date_block_num.mean()) + \\\n",
    "        '\\n  average for all data : ' + \\\n",
    "        str(transactions.date_block_num.mean()) + \\\n",
    "       '\\nlast seen:' + \\\n",
    "       '\\n  exluded shops: ' + \\\n",
    "       str(transactions[transactions.shop_id.isin(excluded_shops)]. \\\n",
    "           groupby('shop_id'). \\\n",
    "           agg({'date_block_num':'max'}).mean().values[0]) +\\\n",
    "       '\\n  excluded items: ' + \\\n",
    "       str(transactions[transactions.item_id.isin(items_train_only)]. \\\n",
    "           groupby('item_id'). \\\n",
    "           agg({'date_block_num':'max'}).mean().values[0]) +\\\n",
    "       '\\n  average for all shops : ' + \\\n",
    "       str(transactions.groupby('shop_id').agg({'date_block_num':'max'}).mean().values[0]) +\\\n",
    "       '\\n  average for all items : ' + \\\n",
    "       str(transactions.groupby('item_id').agg({'date_block_num':'max'}).mean().values[0]) \n",
    "      )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems average date is less for absent shops, so we can somehow conlude the shops not in test set are closed. \n",
    "However, it's not that clear for items, why only about 1/4 of items are in test set. \n",
    "\n",
    "It also doesn't looks like there are only items which was sold, as leaderboard probing shown mean 0.2839\n",
    "https://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/79142\n",
    "\n",
    "There could be a case items excluded are the one's had 0 sales in all the possible shops within the given month (which also suggested by train/test split in week4). The last one can be tested by grouping transactions by date_block_num and count unique items over to see if it's close to test set size\n",
    "\n",
    "This can be utilized together with mean information above during predicting for items not in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6879.764705882353"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.groupby('date_block_num').agg({'item_id':'nunique'}).mean().values[0]\n",
    "\n",
    "## Seems pretty close (espesially if look and number's not mean - 5100 looks reasonable values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "Models are defines in this section to be available to validations (while beeing developed later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_model(train_set, test_set,fit_col):\n",
    "    # we can take global mean of training data, but it's not so easy to calculate, \n",
    "    # as we need to produce proper fraction of zeros. So just constant for some period taken\n",
    "    return np.ones((test_set.shape[0],1),dtype='float32')*0.298 ## true_values['item_cnt_day'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_value_model(train_set, test_set,fit_col):\n",
    "    max_date_block = train_set.date_block_num.max()\n",
    "    \n",
    "    predictions = test_set.merge(train_set[train_set.date_block_num == max_date_block],\\\n",
    "        how='left',on=['shop_id','item_id']).fillna(0.0)\n",
    "    \n",
    "    predictions.loc[predictions.target>20,'target']=20\n",
    "    predictions.loc[predictions.target<0,'target']=0\n",
    "    \n",
    "    return predictions['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_model (train_set, test_set,fit_col):\n",
    "    lr = sklearn.linear_model.LinearRegression(n_jobs=4)\n",
    "    X = train_set[fit_col]\n",
    "    y = train_set['target']\n",
    "    lr.fit(X.values,y.values)\n",
    "    predictions = lr.predict(test_set[fit_col].values)\n",
    "\n",
    "    return predictions.clip(min=0.0, max =20.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_model_ellastic (train_set, test_set,fit_col):\n",
    "    lr = sklearn.linear_model.ElasticNet()\n",
    "    X = train_set[fit_col]\n",
    "    y = train_set['target']\n",
    "    lr.fit(X.values,y.values)\n",
    "    predictions = lr.predict(test_set[fit_col].values)    \n",
    "\n",
    "    return predictions.clip(min=0.0, max =20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_model_ellastic_with_feature_norm (train_set, test_set,fit_col):\n",
    "    ## Seems with MinMaxScaler exactly the same metrics as lin_model. Just, probably given by the \n",
    "    ## fact feature are centered in LinearRegression and not too far from 0-1\n",
    "    ## \n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "    reduced_X_train = pd.DataFrame(train_set[fit_col+['target']],copy=True)\n",
    "    X_train_minmax = scaler.fit_transform(reduced_X_train[fit_col])\n",
    "    reduced_X_train[fit_col] = X_train_minmax\n",
    "\n",
    "    reduced_X_test = pd.DataFrame(test_set[fit_col],copy=True)\n",
    "    X_test_minmax = scaler.transform(reduced_X_test)\n",
    "    reduced_X_test[fit_col] = X_test_minmax\n",
    "\n",
    "    predictions=lin_model_ellastic(reduced_X_train,reduced_X_test,fit_col)\n",
    "    \n",
    "    del reduced_X_train, reduced_X_test, X_train_minmax, X_test_minmax\n",
    "    gc.collect();\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_model(train_set, test_set,fit_col):\n",
    "    xgb_model = xgboost.XGBRegressor(n_jobs=4,objective='reg:squarederror')\n",
    "    xgb_model.fit(train_set[fit_col].values,train_set['target'].values)\n",
    "    predictions = xgb_model.predict(test_set[fit_col].values)\n",
    "    \n",
    "    return predictions.clip(min=0.0, max =20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_model(train_set, test_set,fit_col):\n",
    "    lgb_params = {\n",
    "#                'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':4, \n",
    "#                'min_data_in_leaf': 2**7, \n",
    "#                'bagging_fraction': 0.75, \n",
    "#                'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "#                'bagging_seed': 2**7, \n",
    "#                'num_leaves': 2**7,\n",
    "#                'bagging_freq':1,\n",
    "#                'verbose':0 \n",
    "              }\n",
    "\n",
    "    model = lgb.train(lgb_params, lgb.Dataset(train_set[fit_col].values, label=train_set['target'].values), 100)\n",
    "    predictions = model.predict(test_set[fit_col].values)\n",
    "    return predictions.clip(min=0.0, max =20.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### idea for text features:\n",
    "#### text\n",
    "    1.1. (implemented) word count in items and categories\n",
    "    1.2. number of similar items by description\n",
    "    1.3. (implemented) TFIDF -> SVD \n",
    "    1.4. (implemented) cluster text (item+category) and then cluster number\n",
    "(see https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)\n",
    "    1.5 first word of the shop name is usualy a city. Make it categorical feature?\n",
    "    \n",
    " #### outcome\n",
    "    --> adding 150 SVD features from items description increases memory footprint, while rmse even worse\n",
    "    --> 10 SVD features makes it more feasible, while, it doesn't change RMSE for linear model and makes it worse for \n",
    " lightgbm\n",
    "    --> however, while doing clustering on top of SVD, seems lightgbm slightly benefits from it \n",
    " in this scenario number of SVD components and number of clusters to be turned\n",
    "\n",
    "#### Others\n",
    "\n",
    "    2.1. count of items in given category\n",
    "    2.2. item price (maybe also price changes)\n",
    "    2.3. average category price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_product(*arrays):\n",
    "    la = len(arrays)\n",
    "    dtype = np.result_type(*arrays)\n",
    "    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n",
    "    for i, a in enumerate(np.ix_(*arrays)):\n",
    "        arr[...,i] = a\n",
    "    return arr.reshape(-1, la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_transactions (transactions):\n",
    "    '''\n",
    "    For each dataframe select unique shops and unique items, make all possible pairs out of it \n",
    "    and for each such a pair either aggregate transactions to get sales, or put 0 if no transactions.\n",
    "    The logic was implemented in week4 for validation, also it's the way, how test set is constructed\n",
    "    '''\n",
    "    index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "    grid = [] \n",
    "    for block_num in transactions['date_block_num'].unique():\n",
    "        cur_shops = transactions.loc[transactions['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "        cur_items = transactions.loc[transactions['date_block_num'] == block_num, 'item_id'].unique()\n",
    "        # https://docs.python.org/3/library/itertools.html#itertools.product\n",
    "        # Cartesian product of input iterables.\n",
    "        grid.append(cartesian_product(cur_shops, cur_items, np.array([block_num])))\n",
    "    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "    gb = transactions.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    gb.rename(columns={'item_cnt_day':'target'},inplace=True)\n",
    "    return pd.merge(grid, gb, how='left', on=index_cols).fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_shifts(trainset, testset, fit_cols):\n",
    "    '''\n",
    "    !!!\n",
    "    If this transformation is called - it was tested to be called immidiately after groupping, \n",
    "    otherwise all additional features could be lost?\n",
    "    !!!\n",
    "    \n",
    "    Generate for both train and testset target, shifted by some month as a feature.\n",
    "    testset assumed one date_block_num ahead of train.\n",
    "    '''\n",
    "    shift_range = [1, 2, 3, 4, 5, 12]\n",
    "    \n",
    "    max_block_num = trainset.date_block_num.max()\n",
    "    testset.loc[:,'date_block_num']=max_block_num+1\n",
    "    \n",
    "    ## groups by shop and item\n",
    "    trainset = trainset.merge(\n",
    "        trainset.groupby(['shop_id','date_block_num'],as_index=False).\\\n",
    "            agg({'target':'sum'}).\\\n",
    "            rename(columns={'target':'target_shop'}),\n",
    "        on=['shop_id','date_block_num'], copy=False, validate = 'm:1').fillna(0.0)\n",
    "    trainset = trainset.merge(\n",
    "        trainset.groupby(['item_id','date_block_num'],as_index=False).\\\n",
    "            agg({'target':'sum'}).\\\n",
    "            rename(columns={'target':'target_item'}),\n",
    "        on=['item_id','date_block_num'], copy=False, validate = 'm:1').fillna(0.0)\n",
    "        \n",
    "    ## generating shifts\n",
    "    for shift in shift_range:\n",
    "        curr_fit_cols = ['target_lag_{}'.format(shift),'target_shop_lag_{}'.format(shift),'target_item_lag_{}'.format(shift)]\n",
    "        ##\n",
    "        trainset.loc[:,'shifted_date_block_num'] = trainset['date_block_num']+shift\n",
    "\n",
    "        testset=\\\n",
    "            testset.merge(trainset[['shop_id', 'item_id', 'shifted_date_block_num', 'target','target_shop','target_item']],\n",
    "                      left_on= ['shop_id', 'item_id', 'date_block_num'],\n",
    "                      right_on=['shop_id', 'item_id', 'shifted_date_block_num'], how='left',\n",
    "                      validate='1:1',copy=False,\n",
    "            )[['shop_id', 'item_id', 'date_block_num', 'target','target_shop','target_item']+fit_cols].\\\n",
    "            copy().fillna(0).rename(columns={'target':curr_fit_cols[0],\n",
    "                                      'target_shop':curr_fit_cols[1],\n",
    "                                      'target_item':curr_fit_cols[2]})\n",
    "\n",
    "        trainset=\\\n",
    "            trainset[['shop_id', 'item_id', 'date_block_num', 'target','target_shop','target_item']+fit_cols].merge(\n",
    "            trainset[['shop_id', 'item_id', 'shifted_date_block_num', 'target','target_shop','target_item']],\n",
    "                       left_on= ['shop_id', 'item_id', 'date_block_num'], \n",
    "                        right_on=['shop_id', 'item_id', 'shifted_date_block_num'], how='left',\n",
    "                        suffixes = ('','_shift'), validate='1:1',copy=False,\n",
    "            )[['shop_id', 'item_id', 'date_block_num', 'target','target_shift',\n",
    "               'target_shop','target_item','target_shop_shift','target_item_shift']+fit_cols].\\\n",
    "            copy().fillna(0).rename(columns={'target_shift':curr_fit_cols[0],\n",
    "                                      'target_shop_shift':curr_fit_cols[1],\n",
    "                                      'target_item_shift':curr_fit_cols[2]})\n",
    "        \n",
    "        fit_cols.extend(curr_fit_cols)\n",
    "    \n",
    "    ###\n",
    "    ### should we exclude first monthes ? \n",
    "    ###\n",
    "    ###\n",
    "    gc.collect()\n",
    "    \n",
    "    return (trainset,testset,fit_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_items_SVD_preprocessor(trainset,testset,fit_cols):\n",
    "    ## no need to compute several times. Just compute once, add column to items and check on strat if there is column allready\n",
    "    svd_components_count = 1000\n",
    "    km_clusters = 100\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    item_names_tfidf = vectorizer.fit_transform(items.item_name)\n",
    "    svd = TruncatedSVD(svd_components_count)\n",
    "    item_names_svd = svd.fit_transform(item_names_tfidf)\n",
    "\n",
    "#     print('SDV explained_var ' + str(svd.explained_variance_ratio_.sum()))\n",
    "    km = KMeans(n_clusters=km_clusters, init='k-means++',n_init=3)\n",
    "    km.fit(item_names_svd)\n",
    "\n",
    "    items_with_km = pd.concat([items, pd.DataFrame(km.labels_,columns=['km_item'])],axis=1)\n",
    "\n",
    "    trainset_updated = trainset.merge(items_with_km[['item_id','km_item']], on = 'item_id', how = 'left')\n",
    "    testset_updated = testset.merge(items_with_km[['item_id','km_item']], on = 'item_id', how = 'left')\n",
    "    fit_cols.append('km_item')\n",
    "    \n",
    "    return (trainset_updated, testset_updated, fit_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_text_counts(trainset,testset,fit_cols):\n",
    "    # adding this feature to tree and liner models makes it worse, not better \n",
    "    # word count\n",
    "    items['item_word_count']=items.item_name.apply(lambda x: len(x.split(' '))) \n",
    "    # chars cound\n",
    "    items['item_char_count']=items.item_name.str.len()\n",
    "    \n",
    "    trainset_updated = trainset.merge(items[['item_id','item_word_count','item_char_count']], \n",
    "                                        on = 'item_id', how = 'left')\n",
    "    \n",
    "    testset_updated = testset.merge(items[['item_id','item_word_count','item_char_count']],\n",
    "                                    on = 'item_id', how = 'left')\n",
    "    fit_cols.extend(['item_word_count','item_char_count'])\n",
    "    \n",
    "    return (trainset_updated, testset_updated, fit_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be probably merged with item_text_counts and parametrized..\n",
    "def shop_text_counts(trainset,testset,fit_cols):\n",
    "    # adding this feature to tree and liner models makes it worse, not better\n",
    "    # word count\n",
    "    shops['shop_word_count']=shops.shop_name.apply(lambda x: len(x.split(' '))) \n",
    "    # chars cound\n",
    "    shops['shop_char_count']=shops.shop_name.str.len()\n",
    "    \n",
    "    trainset_updated = trainset.merge(shops[['shop_id','shop_word_count','shop_char_count']], \n",
    "                                        on = 'shop_id', how = 'left')\n",
    "    \n",
    "    testset_updated = testset.merge(shops[['shop_id','shop_word_count','shop_char_count']],\n",
    "                                    on = 'shop_id', how = 'left')\n",
    "    fit_cols.extend(['shop_word_count','shop_char_count'])\n",
    "    \n",
    "    return (trainset_updated, testset_updated, fit_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_id(trainset,testset,fit_cols):\n",
    "    trainset_updated = trainset.merge(items[['item_id','item_category_id']], on = 'item_id', how = 'left')\n",
    "    testset_updated = testset.merge(items[['item_id','item_category_id']], on = 'item_id', how = 'left')\n",
    "    fit_cols.append('item_category_id')\n",
    "    return (trainset_updated, testset_updated, fit_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Before playing with any kind of models validation has to be setup. \n",
    "Some ideas could be taken from week4. \n",
    "\n",
    "In contrast with it 4 cycles of validation to be used - train on date_block_num [0:i] -> validate on [i+1], where i [30,31,32,33]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume every model should take dataset like transations and test pairs (like test_set) and return dataset of prediction \n",
    "[shop_id, item_id, prediction] for the month, next to the last present in dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate (preprocessors, models):\n",
    "    outcome = []\n",
    "    from_block = 30\n",
    "    to_block = 33\n",
    "# testing vals\n",
    "#     from_block = 2\n",
    "#     to_block = 3 \n",
    "\n",
    "    for i in range(from_block,to_block+1):    \n",
    "        print ('validation: generating test and train for i='+str(i))\n",
    "        trainset = group_transactions(transactions[transactions.date_block_num<i])\n",
    "        testset = group_transactions(transactions[transactions.date_block_num==i])\n",
    "        testset_without_targer = testset[['shop_id','item_id']].copy()\n",
    "        \n",
    "#         (train_groupped_with_shifted, test_set_with_shifted, fit_col) = \\\n",
    "#             make_shifts(trainset,testset[['shop_id','item_id']].copy())## copy to avoid SettingWithCopyWarning\n",
    "        applied_preprocessors = []\n",
    "        fit_col=[]\n",
    "        \n",
    "        for preprocessor in preprocessors: \n",
    "            (train_processed, test_processed, fit_col) = \\\n",
    "                preprocessor(trainset,testset_without_targer,fit_col)\n",
    "            \n",
    "            applied_preprocessors.append(preprocessor.__name__)\n",
    "            print('applied_preprocessors: '+str(applied_preprocessors))\n",
    "            \n",
    "            for model in models:\n",
    "                \n",
    "                predictions = model(train_processed, test_processed,fit_col)\n",
    "                # According to competition description validation is done with clipping, so we should do the same\n",
    "                # Model is responsible for it's own clipping if nessesary\n",
    "                # r2 should be as close to 1 as possible. 0 is constant model            \n",
    "#                 r2s = r2_score(testset['target'].clip(lower=0.0, upper =20.0), predictions)\n",
    "                rmse = np.sqrt(mean_squared_error(testset['target'].clip(lower=0.0, upper =20.0), predictions))\n",
    "                print('validation: tested model ' + model.__name__+' rmse = '+str(rmse))\n",
    "#                 outcome.append({'preprocessors':' '.join(applied_preprocessors),\\\n",
    "#                                 'model':model.__name__,'block':i,'metric':'r2s','value':r2s})\n",
    "                outcome.append({'preprocessors':' '.join(applied_preprocessors),\\\n",
    "                                'model':model.__name__,'block':i,'metric':'rmse','value':rmse})\n",
    "            \n",
    "            trainset = train_processed\n",
    "            testset_without_targer = test_processed\n",
    "            \n",
    "        print () \n",
    "        \n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: generating test and train for i=30\n",
      "applied_preprocessors: ['make_shifts']\n",
      "validation: tested model lin_model_ellastic rmse = 0.757732759772218\n",
      "validation: tested model lightgbm_model rmse = 0.7307876680876421\n",
      "applied_preprocessors: ['make_shifts', 'text_items_SVD_preprocessor']\n",
      "validation: tested model lin_model_ellastic rmse = 0.757732759772218\n",
      "validation: tested model lightgbm_model rmse = 0.737034133991881\n",
      "applied_preprocessors: ['make_shifts', 'text_items_SVD_preprocessor', 'category_id']\n",
      "validation: tested model lin_model_ellastic rmse = 0.757732759772218\n",
      "validation: tested model lightgbm_model rmse = 0.7373853021542082\n",
      "\n",
      "validation: generating test and train for i=31\n",
      "applied_preprocessors: ['make_shifts']\n",
      "validation: tested model lin_model_ellastic rmse = 0.8381509092447855\n",
      "validation: tested model lightgbm_model rmse = 0.8161690531405288\n",
      "applied_preprocessors: ['make_shifts', 'text_items_SVD_preprocessor']\n",
      "validation: tested model lin_model_ellastic rmse = 0.8381509092447855\n",
      "validation: tested model lightgbm_model rmse = 0.8112803996593726\n",
      "applied_preprocessors: ['make_shifts', 'text_items_SVD_preprocessor', 'category_id']\n",
      "validation: tested model lin_model_ellastic rmse = 0.8381509092447855\n",
      "validation: tested model lightgbm_model rmse = 0.8042946161272642\n",
      "\n",
      "validation: generating test and train for i=32\n",
      "applied_preprocessors: ['make_shifts']\n",
      "validation: tested model lin_model_ellastic rmse = 0.984685870076126\n",
      "validation: tested model lightgbm_model rmse = 0.9403533959394279\n",
      "applied_preprocessors: ['make_shifts', 'text_items_SVD_preprocessor']\n",
      "validation: tested model lin_model_ellastic rmse = 0.984685870076126\n",
      "validation: tested model lightgbm_model rmse = 0.9277661193529824\n",
      "applied_preprocessors: ['make_shifts', 'text_items_SVD_preprocessor', 'category_id']\n",
      "validation: tested model lin_model_ellastic rmse = 0.984685870076126\n",
      "validation: tested model lightgbm_model rmse = 0.9068968475545014\n",
      "\n",
      "validation: generating test and train for i=33\n",
      "applied_preprocessors: ['make_shifts']\n",
      "validation: tested model lin_model_ellastic rmse = 1.000706276704384\n",
      "validation: tested model lightgbm_model rmse = 0.9761707376980482\n",
      "applied_preprocessors: ['make_shifts', 'text_items_SVD_preprocessor']\n",
      "validation: tested model lin_model_ellastic rmse = 1.000706276704384\n",
      "validation: tested model lightgbm_model rmse = 0.977176754707354\n",
      "applied_preprocessors: ['make_shifts', 'text_items_SVD_preprocessor', 'category_id']\n",
      "validation: tested model lin_model_ellastic rmse = 1.000706276704384\n",
      "validation: tested model lightgbm_model rmse = 0.9697529353440704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# resut = validate([constant_model,previous_value_model,lin_model,xgboost_model])\n",
    "# resut = validate([constant_model,previous_value_model,lin_model, lin_model_with_feature_norm, lightgbm_model])\n",
    "\n",
    "resut = validate(\n",
    "    [make_shifts, text_items_SVD_preprocessor,category_id],\n",
    "    [lin_model_ellastic,lightgbm_model]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>preprocessors</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lightgbm_model</td>\n",
       "      <td>make_shifts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.865870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lightgbm_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.863314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lightgbm_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor category_id</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.854582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lin_model_ellastic</td>\n",
       "      <td>make_shifts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.895319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lin_model_ellastic</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.895319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lin_model_ellastic</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor category_id</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.895319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model                                        preprocessors  \\\n",
       "0      lightgbm_model                                          make_shifts   \n",
       "1      lightgbm_model              make_shifts text_items_SVD_preprocessor   \n",
       "2      lightgbm_model  make_shifts text_items_SVD_preprocessor category_id   \n",
       "3  lin_model_ellastic                                          make_shifts   \n",
       "4  lin_model_ellastic              make_shifts text_items_SVD_preprocessor   \n",
       "5  lin_model_ellastic  make_shifts text_items_SVD_preprocessor category_id   \n",
       "\n",
       "  metric     value  \n",
       "0   rmse  0.865870  \n",
       "1   rmse  0.863314  \n",
       "2   rmse  0.854582  \n",
       "3   rmse  0.895319  \n",
       "4   rmse  0.895319  \n",
       "5   rmse  0.895319  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 150\n",
    "pd.DataFrame(resut).groupby(['model','preprocessors','metric'],as_index=False).agg({'value':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>preprocessors</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lightgbm_model</td>\n",
       "      <td>make_shifts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.865870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lightgbm_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.861515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lightgbm_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor item_text_counts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.865068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lightgbm_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor item_text_counts shop_text_counts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.865586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lin_model</td>\n",
       "      <td>make_shifts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.904058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lin_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.904204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lin_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor item_text_counts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.904330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lin_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor item_text_counts shop_text_counts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.904466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lin_model_ellastic</td>\n",
       "      <td>make_shifts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.895319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lin_model_ellastic</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.895319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lin_model_ellastic</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor item_text_counts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.895319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lin_model_ellastic</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor item_text_counts shop_text_counts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.895319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lin_model_ellastic_with_feature_norm</td>\n",
       "      <td>make_shifts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.896609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lin_model_ellastic_with_feature_norm</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.896609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lin_model_ellastic_with_feature_norm</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor item_text_counts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.896609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lin_model_ellastic_with_feature_norm</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor item_text_counts shop_text_counts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.896609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>previous_value_model</td>\n",
       "      <td>make_shifts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>1.008575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>previous_value_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor</td>\n",
       "      <td>rmse</td>\n",
       "      <td>1.008575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>previous_value_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor item_text_counts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>1.008575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>previous_value_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor item_text_counts shop_text_counts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>1.008575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xgboost_model</td>\n",
       "      <td>make_shifts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.876752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>xgboost_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.875871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>xgboost_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor item_text_counts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.876341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>xgboost_model</td>\n",
       "      <td>make_shifts text_items_SVD_preprocessor item_text_counts shop_text_counts</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.877762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   model  \\\n",
       "0                         lightgbm_model   \n",
       "1                         lightgbm_model   \n",
       "2                         lightgbm_model   \n",
       "3                         lightgbm_model   \n",
       "4                              lin_model   \n",
       "5                              lin_model   \n",
       "6                              lin_model   \n",
       "7                              lin_model   \n",
       "8                     lin_model_ellastic   \n",
       "9                     lin_model_ellastic   \n",
       "10                    lin_model_ellastic   \n",
       "11                    lin_model_ellastic   \n",
       "12  lin_model_ellastic_with_feature_norm   \n",
       "13  lin_model_ellastic_with_feature_norm   \n",
       "14  lin_model_ellastic_with_feature_norm   \n",
       "15  lin_model_ellastic_with_feature_norm   \n",
       "16                  previous_value_model   \n",
       "17                  previous_value_model   \n",
       "18                  previous_value_model   \n",
       "19                  previous_value_model   \n",
       "20                         xgboost_model   \n",
       "21                         xgboost_model   \n",
       "22                         xgboost_model   \n",
       "23                         xgboost_model   \n",
       "\n",
       "                                                                preprocessors  \\\n",
       "0                                                                 make_shifts   \n",
       "1                                     make_shifts text_items_SVD_preprocessor   \n",
       "2                    make_shifts text_items_SVD_preprocessor item_text_counts   \n",
       "3   make_shifts text_items_SVD_preprocessor item_text_counts shop_text_counts   \n",
       "4                                                                 make_shifts   \n",
       "5                                     make_shifts text_items_SVD_preprocessor   \n",
       "6                    make_shifts text_items_SVD_preprocessor item_text_counts   \n",
       "7   make_shifts text_items_SVD_preprocessor item_text_counts shop_text_counts   \n",
       "8                                                                 make_shifts   \n",
       "9                                     make_shifts text_items_SVD_preprocessor   \n",
       "10                   make_shifts text_items_SVD_preprocessor item_text_counts   \n",
       "11  make_shifts text_items_SVD_preprocessor item_text_counts shop_text_counts   \n",
       "12                                                                make_shifts   \n",
       "13                                    make_shifts text_items_SVD_preprocessor   \n",
       "14                   make_shifts text_items_SVD_preprocessor item_text_counts   \n",
       "15  make_shifts text_items_SVD_preprocessor item_text_counts shop_text_counts   \n",
       "16                                                                make_shifts   \n",
       "17                                    make_shifts text_items_SVD_preprocessor   \n",
       "18                   make_shifts text_items_SVD_preprocessor item_text_counts   \n",
       "19  make_shifts text_items_SVD_preprocessor item_text_counts shop_text_counts   \n",
       "20                                                                make_shifts   \n",
       "21                                    make_shifts text_items_SVD_preprocessor   \n",
       "22                   make_shifts text_items_SVD_preprocessor item_text_counts   \n",
       "23  make_shifts text_items_SVD_preprocessor item_text_counts shop_text_counts   \n",
       "\n",
       "   metric     value  \n",
       "0    rmse  0.865870  \n",
       "1    rmse  0.861515  \n",
       "2    rmse  0.865068  \n",
       "3    rmse  0.865586  \n",
       "4    rmse  0.904058  \n",
       "5    rmse  0.904204  \n",
       "6    rmse  0.904330  \n",
       "7    rmse  0.904466  \n",
       "8    rmse  0.895319  \n",
       "9    rmse  0.895319  \n",
       "10   rmse  0.895319  \n",
       "11   rmse  0.895319  \n",
       "12   rmse  0.896609  \n",
       "13   rmse  0.896609  \n",
       "14   rmse  0.896609  \n",
       "15   rmse  0.896609  \n",
       "16   rmse  1.008575  \n",
       "17   rmse  1.008575  \n",
       "18   rmse  1.008575  \n",
       "19   rmse  1.008575  \n",
       "20   rmse  0.876752  \n",
       "21   rmse  0.875871  \n",
       "22   rmse  0.876341  \n",
       "23   rmse  0.877762  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 150\n",
    "pd.DataFrame(resut).groupby(['model','preprocessors','metric'],as_index=False).agg({'value':'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation:\n",
    "\n",
    "|preprocessors|model| \tmetric \t|value|\n",
    "|----|----|-----|-----------|\n",
    "|make_shifts+text_items_SVD_preprocessor|constant_model|rmse|1.086481|\n",
    "|make_shifts+text_items_SVD_preprocessor|lin_model|rmse|0.904204|\n",
    "|make_shifts+text_items_SVD_preprocessor|previous_value_model|rmse|1.008575|\n",
    "|make_shifts+text_items_SVD_preprocessor|xgboost_model|rmse|0.875871|\n",
    "|make_shifts+text_items_SVD_preprocessor|lightgbm_model|rmse|0.861515|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Seems local validation somehow corelates with public leaderboard </b>\n",
    "\n",
    "|Name|Score|Local score|Model|Comment|\n",
    "|----|-----|-----------|-----|-------|\n",
    "|submition_2019_06_03_09_29.csv.gz|1.16777|1.008575|Last month baseline||\n",
    "|submition_lin_model_2019_12_02_13_50.csv.gz|1.96182||simple linear model with 2 month shifts and with clipping 0-300|wrong cliipng|\n",
    "|submition_lin_model_2019_12_02_13_56.csv.gz|4.34576||simple linear model with 2 month shifts with clipping removed|no clipping|\n",
    "|submition_lin_model_2019_12_02_15_39.csv.gz|1.08014|0.905361|simple linear model, more with proper clipping|proper clipping apllied|\n",
    "|submition_xgboost_model_2019_12_03_15_43.csv.gz|1.02016|0.870930|simple xgboost without any parameters tuned|\n",
    "|submition_lightgbm_model_2019_12_04_05_53.csv.gz|**1.00715**|0.862635|simple light gbm without any parameters tuned|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_submit(model):\n",
    "    \n",
    "    trainset = group_transactions(transactions) # <-using global var in func \n",
    "    \n",
    "    (train_groupped_with_shifted, test_set_with_shifted, fit_col) = \\\n",
    "            make_shifts(trainset,test_set) # <-using global var in func \n",
    "    \n",
    "    predictions = model(train_groupped_with_shifted, test_set_with_shifted,fit_col)\n",
    "    \n",
    "    test_set_with_shifted.loc[:,'predictions'] = predictions.astype(float)\n",
    "    test_set_with_shifted.loc[:,'ID'] = test_set.ID.astype(int)        \n",
    "    test_set_with_shifted.rename(columns={'predictions':'item_cnt_month'},inplace=True)\n",
    "    \n",
    "\n",
    "    filename = 'submition_{}_{}.csv.gz'.format(model.__name__,time.strftime('%Y_%m_%d_%H_%M',time.localtime()))\n",
    "    test_set_with_shifted[['ID','item_cnt_month']].to_csv(filename,index=False)\n",
    "    print (filename + ' prepared')\n",
    "    \n",
    "    \n",
    "# prepare_submit(lightgbm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: TODO\n",
    "1. From the task - Type of public/private split is identified\n",
    "2. Compare for more complex model which metric better correlates with lederboard - rmse or r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trainset = group_transactions(transactions[transactions.date_block_num<13])\n",
    "# testset = group_transactions(transactions[transactions.date_block_num==13])\n",
    "# testset_without_targer = testset[['shop_id','item_id']].copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From tds article\n",
    "https://colab.research.google.com/drive/1lUwtvOInzoaNC5eBMljRMVk1K9zcKD-b\n",
    "\n",
    "https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "\n",
    "seems routines used for classic models doesn't makes sence for LSTM. \n",
    "The logic for lstm should be \n",
    "\n",
    "0. generate pairs (item_id, shop_id) from all the items met in the test_set and 3 shops (for testing)\n",
    "1. select time_window = 24 (2 years), n_features = 1 (sales last_month) \n",
    "\n",
    "2. generate train sequences in a way that each sequence [samples,time_window,n_features]\n",
    "   \n",
    "    \n",
    "3. split dev_train and dev_test in a way [dev_train_X:dev_train_Y], \n",
    "   where dev_train_X  = (first n frames sales data) and dev_train_Y - last sales \n",
    "\n",
    "3. consider adding additional features later from text_items_SVD_preprocessor for example\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_block=30\n",
    "\n",
    "# shop_ids = np.random.choice(transactions[transactions.date_block_num==train_test_split_block].shop_id.unique(),10)\n",
    "shop_ids = transactions[transactions.date_block_num==train_test_split_block].shop_id.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_window = 24\n",
    "time_window = 13\n",
    "n_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "train_X = []\n",
    "train_Y = []\n",
    "\n",
    "for cur_seq_window in range(time_window,train_test_split_block):\n",
    "    start_interval = cur_seq_window-time_window\n",
    "#     print(start_interval,cur_seq_window)\n",
    "    #item_ids from Y\n",
    "    item_ids = transactions[\n",
    "        (transactions.date_block_num==cur_seq_window) &\n",
    "        (transactions.shop_id.isin(shop_ids))\n",
    "    ].item_id.unique()\n",
    "    # grid for both X and Y\n",
    "    v_grid_X = \\\n",
    "        cartesian_product(shop_ids, item_ids, np.array(range(cur_seq_window-time_window,cur_seq_window)))    \n",
    "    pd_grid_X = pd.DataFrame(v_grid_X, columns = index_cols,dtype=np.int32)\n",
    "    \n",
    "    X_trans = transactions[\n",
    "        (transactions.date_block_num>=start_interval) &\n",
    "        (transactions.date_block_num<cur_seq_window) &\n",
    "        (transactions.shop_id.isin(shop_ids)) & \n",
    "        (transactions.item_id.isin(item_ids))\n",
    "    ]\n",
    "    gb_X = X_trans.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    pd_grid_val_X = pd.merge(pd_grid_X, gb_X, how='left', on=index_cols).fillna(0.0)    \n",
    "    seq_num = int(pd_grid_val_X.shape[0]/time_window)\n",
    "#     print (\"seq_num = \" + str(seq_num))    \n",
    "    np_X_chunk = np.reshape(pd_grid_val_X.sort_values(by=index_cols)['item_cnt_day'].values,\n",
    "                                  (seq_num,time_window,n_features))\n",
    "    \n",
    "    \n",
    "    v_grid_Y = cartesian_product(shop_ids, item_ids, np.array([cur_seq_window]))\n",
    "    pd_grid_Y = pd.DataFrame(v_grid_Y, columns = index_cols,dtype=np.int32)    \n",
    "    Y_trans = transactions[\n",
    "        (transactions.date_block_num==cur_seq_window) &\n",
    "        (transactions.shop_id.isin(shop_ids)) & \n",
    "        (transactions.item_id.isin(item_ids))\n",
    "    ]\n",
    "    gb_Y = Y_trans.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\n",
    "    pd_grid_val_Y = pd.merge(pd_grid_Y, gb_Y, how='left', on=index_cols).fillna(0.0)\n",
    "    \n",
    "    np_Y_chunk = pd_grid_val_Y.sort_values(by=index_cols)['item_cnt_day'].values\n",
    "    \n",
    "#     print (\"np_X_chunk.shape = \"+str(np_X_chunk.shape)+\"np_Y_chunk.shape = \"+str(np_Y_chunk.shape))\n",
    "\n",
    "\n",
    "    \n",
    "    if (cur_seq_window==train_test_split_block-1):\n",
    "        ## test set\n",
    "        test_X = np_X_chunk\n",
    "        test_Y = np_Y_chunk\n",
    "    else:\n",
    "        ## train set\n",
    "        train_X.append(np_X_chunk)\n",
    "        train_Y.append(np_Y_chunk)\n",
    "        \n",
    "train_X_final = np.vstack(train_X)\n",
    "train_Y_final = np.hstack(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4221912, 13, 1) (4221912,) (224288, 13, 1) (224288,)\n"
     ]
    }
   ],
   "source": [
    "print(train_X_final.shape , train_Y_final.shape, test_X.shape , test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 24/30\n",
    "# 1228682/1228682 - 126s - loss: 10.3635 - val_loss: 2.6240\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(32, input_shape=(train_X_final.shape[1], train_X_final.shape[2]),dropout = 0.4))\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam())\n",
    "\n",
    "history = model.fit(\n",
    "    train_X_final, train_Y_final, \n",
    "    epochs=30, \n",
    "    batch_size=128, \n",
    "    validation_data=(test_X, test_Y),\n",
    "    verbose=2, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64 units\n",
    "# Epoch 22/30\n",
    "# 1228682/1228682 - 211s - loss: 10.1696 - val_loss: 2.4526\n",
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.LSTM(64, input_shape=(train_X_final.shape[1], train_X_final.shape[2])))\n",
    "model2.add(keras.layers.Dense(1))\n",
    "model2.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam())\n",
    "\n",
    "history2 = model2.fit(\n",
    "    train_X_final, train_Y_final, \n",
    "    epochs=30, \n",
    "    batch_size=128, \n",
    "    validation_data=(test_X, test_Y),\n",
    "    verbose=2, \n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hw_version": "1.0.0",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
